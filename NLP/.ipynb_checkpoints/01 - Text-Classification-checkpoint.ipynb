{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T19:53:11.370917Z",
     "start_time": "2020-09-17T19:53:08.381264Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5XgKTMRwUHu6"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string, random, re, requests, os, textwrap\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T19:53:11.600671Z",
     "start_time": "2020-09-17T19:53:11.373006Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zekinchangmail.com/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/zekinchangmail.com/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/zekinchangmail.com/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detection (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T19:53:11.648166Z",
     "start_time": "2020-09-17T19:53:11.605482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n",
      "ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "ham\tOk lar... Joking wif u oni...\n"
     ]
    }
   ],
   "source": [
    "# nltk.download_shell()\n",
    "messages = [line.rstrip() for line in open('./data/SMSSpamCollection')]\n",
    "print(len(messages))\n",
    "for msg in messages[:2]:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T19:53:11.697047Z",
     "start_time": "2020-09-17T19:53:11.651869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = pd.read_csv('./data/SMSSpamCollection', sep='\\t', names=[\"label\", \"message\"])\n",
    "messages['length'] = messages['message'].apply(len)\n",
    "print(messages.shape)\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T19:53:11.773103Z",
     "start_time": "2020-09-17T19:53:11.701746Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825.0</td>\n",
       "      <td>71.482487</td>\n",
       "      <td>58.440652</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>910.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747.0</td>\n",
       "      <td>138.670683</td>\n",
       "      <td>28.873603</td>\n",
       "      <td>13.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>223.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       length                                                         \n",
       "        count        mean        std   min    25%    50%    75%    max\n",
       "label                                                                 \n",
       "ham    4825.0   71.482487  58.440652   2.0   33.0   52.0   93.0  910.0\n",
       "spam    747.0  138.670683  28.873603  13.0  133.0  149.0  157.0  223.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T19:53:12.349976Z",
     "start_time": "2020-09-17T19:53:11.782305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages.hist(column=['length'], bins=50, by='label', figsize=(15,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T19:53:12.361130Z",
     "start_time": "2020-09-17T19:53:12.353267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sample message Notice it has punctuation'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove punctuation\n",
    "mess = 'Sample message! Notice: it has punctuation.'\n",
    "print(string.punctuation)\n",
    "nopunc = [char for char in mess if char not in string.punctuation]\n",
    "nopunc = ''.join(nopunc)\n",
    "nopunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T19:53:12.372289Z",
     "start_time": "2020-09-17T19:53:12.366532Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T19:53:13.695058Z",
     "start_time": "2020-09-17T19:53:13.670083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Go, jurong, point, crazy, Available, bugis, n...\n",
       "1                       [Ok, lar, Joking, wif, u, oni]\n",
       "2    [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
       "3        [U, dun, say, early, hor, U, c, already, say]\n",
       "4    [Nah, dont, think, goes, usf, lives, around, t...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['message'].head().apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T20:02:21.479699Z",
     "start_time": "2020-09-17T20:02:21.469749Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove row with all space\n",
    "messages = messages[~messages['message'].str.isspace()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T20:02:21.976001Z",
     "start_time": "2020-09-17T20:02:21.964879Z"
    }
   },
   "outputs": [],
   "source": [
    "# train test split \n",
    "msg_train, msg_test, label_train, label_test = train_test_split(messages['message'], \n",
    "                                                                messages['label'], \n",
    "                                                                test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T15:51:51.292282Z",
     "start_time": "2020-08-12T15:51:51.282247Z"
    }
   },
   "source": [
    "**Vectorization**\n",
    "\n",
    "1. Count how many times does a word occur in each message (Known as term frequency)\n",
    "\n",
    "2. Weigh the counts, so that frequent tokens get lower weight (inverse document frequency)\n",
    "\n",
    "3. Normalize the vectors to unit length, to abstract from the original text length (L2 norm)\n",
    "\n",
    "**TF: Term Frequency**, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length.\n",
    "\n",
    "<b>*TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).*</b>\n",
    "\n",
    "**IDF: Inverse Document Frequency**, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: \n",
    "\n",
    "<b>*IDF(t) = log_e(Total number of documents / Number of documents with term t in it).*</b>\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider a document containing 100 words wherein the word cat appears 3 times. <br/>\n",
    "The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. <br/>Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T20:02:22.367379Z",
     "start_time": "2020-09-17T20:02:22.363616Z"
    }
   },
   "outputs": [],
   "source": [
    "# we can do it in the pipelines\n",
    "# pipeline = Pipeline([\n",
    "#     ('tfidf', TfidfVectorizer()),  # integer counts to weighted TF-IDF scores\n",
    "#     ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "# ])\n",
    "\n",
    "# pipeline.fit(msg_train,label_train)\n",
    "# predictions = pipeline.predict(msg_test)\n",
    "# print(classification_report(predictions,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T20:02:22.727839Z",
     "start_time": "2020-09-17T20:02:22.562861Z"
    }
   },
   "outputs": [],
   "source": [
    "# term frequency, term counts per documents/rows\n",
    "# vectorizer = CountVectorizer(analyzer=text_process)\n",
    "# msg_train = vectorizer.fit_transform(msg_train)\n",
    "# msg_test = vectorizer.transform(msg_test)\n",
    "# vectorizer.vocabulary['bishan']\n",
    "# print(len(vectorizer.vocabulary))\n",
    "\n",
    "# TF-IDF scores\n",
    "# transformer = TfidfTransformer()\n",
    "# msg_train = transformer.fit_transform(msg_train)\n",
    "# msg_test = transformer.transform(msg_test)\n",
    "# print(transformer.idf_[vectorizer.vocabulary_['u']])\n",
    "# print(transformer.idf_[vectorizer.vocabulary_['bishan']])\n",
    "\n",
    "# Combine above 2 steps\n",
    "tf_vectorizer = TfidfVectorizer()\n",
    "msg_train = tf_vectorizer.fit_transform(msg_train)\n",
    "msg_test = tf_vectorizer.transform(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T20:02:22.770763Z",
     "start_time": "2020-09-17T20:02:22.757613Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4982)\t0.6248128474881275\n",
      "  (0, 3377)\t0.23918601245323678\n",
      "  (0, 2453)\t0.335393150298881\n",
      "  (0, 7408)\t0.271849423277969\n",
      "  (0, 6495)\t0.3264512001316896\n",
      "  (0, 3476)\t0.509350465582496\n",
      "  (1, 3215)\t0.3656287860416996\n",
      "  (1, 2400)\t0.31950752565453244\n",
      "  (1, 6935)\t0.09179147005093953\n",
      "  (1, 6835)\t0.17672056460754845\n",
      "  (1, 963)\t0.2304959356230266\n",
      "  (1, 7595)\t0.2711164486848109\n",
      "  (1, 2449)\t0.19855625671457314\n",
      "  (1, 5986)\t0.31950752565453244\n",
      "  (1, 1077)\t0.1483268751842984\n",
      "  (1, 2814)\t0.27833095969025845\n",
      "  (1, 3545)\t0.16360664413187753\n",
      "  (1, 6002)\t0.19006054821069723\n",
      "  (1, 4427)\t0.2565118012488886\n",
      "  (1, 7543)\t0.2810403758956763\n",
      "  (1, 4120)\t0.22649468271001558\n",
      "  (1, 3641)\t0.31303606732796385\n",
      "  (2, 1643)\t0.32181066644340994\n",
      "  (2, 1666)\t0.3453450722131478\n",
      "  (2, 4872)\t0.6691860648832222\n",
      "  :\t:\n",
      "  (4453, 3285)\t0.5000010719576856\n",
      "  (4453, 4272)\t0.5012592088776577\n",
      "  (4454, 4232)\t0.3101101979534473\n",
      "  (4454, 2256)\t0.32123935849985547\n",
      "  (4454, 1302)\t0.32123935849985547\n",
      "  (4454, 2072)\t0.32123935849985547\n",
      "  (4454, 3261)\t0.32123935849985547\n",
      "  (4454, 2552)\t0.2944245271514934\n",
      "  (4454, 3182)\t0.28846111188020274\n",
      "  (4454, 6737)\t0.2709757852352212\n",
      "  (4454, 3402)\t0.22619900161277454\n",
      "  (4454, 6785)\t0.2519240250011775\n",
      "  (4454, 4960)\t0.13313464817631074\n",
      "  (4454, 4921)\t0.12897902880577541\n",
      "  (4454, 3765)\t0.11614190628294283\n",
      "  (4454, 6819)\t0.10499668361290408\n",
      "  (4454, 6935)\t0.08458536339926416\n",
      "  (4454, 7543)\t0.2589772482324273\n",
      "  (4455, 5707)\t0.41823369809013355\n",
      "  (4455, 7662)\t0.7759463921430176\n",
      "  (4455, 4939)\t0.3429912919818339\n",
      "  (4455, 2178)\t0.32456855042766786\n",
      "  (4456, 5239)\t0.809766105104768\n",
      "  (4456, 7194)\t0.4083502671338532\n",
      "  (4456, 7467)\t0.4213418022878392\n"
     ]
    }
   ],
   "source": [
    "# TF - IDF of train data\n",
    "print(msg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T20:02:22.996407Z",
     "start_time": "2020-09-17T20:02:22.965089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'none'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer.get_feature_names()[4840]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T20:02:24.222941Z",
     "start_time": "2020-09-17T20:02:23.497154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate for NB: 0.968609865470852\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier()\n",
    "model.fit(msg_train, label_train)\n",
    "print(\"Classification rate for NB:\", model.score(msg_test, label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T10:10:53.849744Z",
     "start_time": "2020-08-15T10:10:53.776912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[966  26]\n",
      " [  3 120]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.97      0.99       992\n",
      "        spam       0.82      0.98      0.89       123\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.91      0.97      0.94      1115\n",
      "weighted avg       0.98      0.97      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(msg_test)\n",
    "print(confusion_matrix(pred, label_test))\n",
    "print(classification_report(pred, label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T10:11:03.147480Z",
     "start_time": "2020-08-15T10:11:03.123597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You won luckydraw, claim your prize at Bishan Now.']\n",
      "\n",
      "This message is ['spam']\n"
     ]
    }
   ],
   "source": [
    "msg_val = ['You won luckydraw, claim your prize at Bishan Now.']\n",
    "print(msg_val)\n",
    "msg_val = tf_vectorizer.transform(msg_val)\n",
    "pred = model.predict(msg_val)\n",
    "print(f'\\nThis message is {pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:41:39.336864Z",
     "start_time": "2020-08-13T06:41:39.331816Z"
    }
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word_index_map = {}\n",
    "current_index = 0\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:41:44.070351Z",
     "start_time": "2020-08-13T06:41:40.332469Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_reviews = BeautifulSoup(open('./data/positive.review').read(), features=\"html5lib\")\n",
    "positive_reviews = positive_reviews.findAll('review_text')\n",
    "\n",
    "negative_reviews = BeautifulSoup(open('./data/negative.review').read(), features=\"html5lib\")\n",
    "negative_reviews = negative_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:41:39.767226Z",
     "start_time": "2020-08-13T06:41:39.751094Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_tokenizer(s):\n",
    "    '''To tokenize sentence into list'''\n",
    "    # split string into words (tokens)\n",
    "    tokens = nltk.tokenize.word_tokenize(s.lower())\n",
    "    # remove short words, they're probably not useful\n",
    "    tokens = [t for t in tokens if len(t) > 2] \n",
    "    # put words into base form, dogs => dog, jumping => jump\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] \n",
    "    # remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stopwords.words('english')] \n",
    "    # remove punctuction\n",
    "    tokens = [t for t in tokens if t not in string.punctuation]\n",
    "    return tokens\n",
    "\n",
    "def get_token(review):\n",
    "    '''To update full map of the term in corpus'''\n",
    "    global word_index_map, current_index\n",
    "    \n",
    "    tokens = my_tokenizer(review.text)\n",
    "    # update word_map\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "    return tokens\n",
    "\n",
    "def tokens_to_vector(tokens):\n",
    "    '''To obtain term frequency'''\n",
    "    x = np.zeros(len(word_index_map)) # last element is for the label\n",
    "    for t in tokens:\n",
    "        x[word_index_map[t]] += 1\n",
    "    # normalized\n",
    "    x = x / x.sum()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:42:30.989838Z",
     "start_time": "2020-08-13T06:41:44.074111Z"
    }
   },
   "outputs": [],
   "source": [
    "for review in positive_reviews:\n",
    "    tokens = get_token(review)\n",
    "    positive_tokenized.append(tokens)\n",
    "    \n",
    "for review in negative_reviews:\n",
    "    tokens = get_token(review)\n",
    "    negative_tokenized.append(tokens)\n",
    "    \n",
    "print(\"len(word_index_map):\", len(word_index_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:42:31.554958Z",
     "start_time": "2020-08-13T06:42:30.992389Z"
    }
   },
   "outputs": [],
   "source": [
    "# label 1 for positive, 0 for negative\n",
    "y = np.hstack([np.ones(len(positive_tokenized)), np.zeros(len(negative_tokenized))])\n",
    "full_data = positive_tokenized + negative_tokenized\n",
    "data = np.zeros((len(full_data), len(word_index_map)))\n",
    "i = 0\n",
    "\n",
    "for tokens in full_data:\n",
    "    xy = tokens_to_vector(tokens)\n",
    "    data[i, :] = xy\n",
    "    i += 1\n",
    "\n",
    "# stack y varaibles on it\n",
    "data = np.hstack([data, y.reshape(-1, 1)])\n",
    "    \n",
    "data = shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:42:31.675419Z",
     "start_time": "2020-08-13T06:42:31.556884Z"
    }
   },
   "outputs": [],
   "source": [
    "# train test split \n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(data[:, :-1], \n",
    "                                                data[:, -1], \n",
    "                                                test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:42:31.978139Z",
     "start_time": "2020-08-13T06:42:31.677184Z"
    }
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"Train accuracy:\", model.score(Xtrain, Ytrain))\n",
    "print(\"Test accuracy:\", model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:42:32.019899Z",
     "start_time": "2020-08-13T06:42:31.983765Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check words that far away from zero, important to defined good or bad\n",
    "threshold = 0.5\n",
    "for word, index in word_index_map.items():\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        print(word, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cipher Decryption (GA and Bigram Prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T10:21:35.018992Z",
     "start_time": "2020-08-12T10:21:35.012487Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "sEUrJ4J_UU-w"
   },
   "outputs": [],
   "source": [
    "# download the file\n",
    "if not os.path.exists('./data/moby_dick.txt'):\n",
    "    r = requests.get('https://lazyprogrammer.me/course_files/moby_dick.txt')\n",
    "    with open('./data/moby_dick.txt', 'w') as f:\n",
    "        f.write(r.content.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T10:21:35.224130Z",
     "start_time": "2020-08-12T10:21:35.212580Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "zMrtqw5DUNp3"
   },
   "outputs": [],
   "source": [
    "# one will act as the key, other as the value\n",
    "letters1 = list(string.ascii_lowercase)\n",
    "letters2 = list(string.ascii_lowercase)\n",
    "random.shuffle(letters2)\n",
    "true_mapping = {k : v for k, v in zip(letters1, letters2)}\n",
    "# unigram prob\n",
    "pi = np.zeros(26)\n",
    "# bigram prob\n",
    "M_bi = np.ones((26, 26))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T10:21:35.465968Z",
     "start_time": "2020-08-12T10:21:35.451861Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "IyAGzd5iUQ_W"
   },
   "outputs": [],
   "source": [
    "def update_pi(ch):\n",
    "    '''To update the initial state distribution for first letter, unigram'''\n",
    "    i = ord(ch) - 97\n",
    "    pi[i] += 1\n",
    "\n",
    "def update_transition(ch1, ch2):\n",
    "    '''To update the Markov matrix, bigram'''\n",
    "    # ord('a') = 97, ord('b') = 98, ...\n",
    "    i = ord(ch1) - 97\n",
    "    j = ord(ch2) - 97\n",
    "    M_bi[i, j] += 1\n",
    "\n",
    "def get_word_prob(word):\n",
    "    '''to get the log-probability of a word / token'''\n",
    "    i = ord(word[0]) - 97\n",
    "    logp = np.log(pi[i])\n",
    "    \n",
    "    for ch in word[1:]:\n",
    "        j = ord(ch) - 97\n",
    "        logp += np.log(M_bi[i, j])\n",
    "        i = j\n",
    "\n",
    "    return logp\n",
    "\n",
    "def get_sequence_prob(words):\n",
    "    '''To get the probability of a sequence of words'''\n",
    "    # if input is a string, split into an array of tokens\n",
    "    if type(words) == str:\n",
    "        words = words.split()\n",
    "    logp = 0\n",
    "    for word in words:\n",
    "        logp += get_word_prob(word)\n",
    "    return logp\n",
    "\n",
    "def encode_message(msg):\n",
    "    '''encode Message'''\n",
    "    msg = msg.lower()\n",
    "    msg = regex.sub(' ', msg)\n",
    "\n",
    "  # make the encoded message\n",
    "    coded_msg = []\n",
    "    for ch in msg:\n",
    "        # if character is non-alpha then it remains\n",
    "        if ch in true_mapping:\n",
    "            coded_ch = true_mapping[ch]\n",
    "        else:\n",
    "            coded_ch = ch\n",
    "        coded_msg.append(coded_ch)\n",
    "    return ''.join(coded_msg)\n",
    "\n",
    "# a function to decode a message\n",
    "def decode_message(msg, word_map):\n",
    "    '''decode Message'''\n",
    "    decoded_msg = []\n",
    "    for ch in msg:\n",
    "        if ch in word_map:\n",
    "            decoded_ch = word_map[ch]\n",
    "        else:\n",
    "            decoded_ch = ch\n",
    "        decoded_msg.append(decoded_ch)\n",
    "\n",
    "    return ''.join(decoded_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Model Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T10:21:37.282918Z",
     "start_time": "2020-08-12T10:21:36.111468Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "PxacIxtyUZFd"
   },
   "outputs": [],
   "source": [
    "# training for probabilities\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "for line in open('./data/moby_dick.txt'):\n",
    "    line = line.rstrip()\n",
    "    \n",
    "    # if line is not blank, replace non-alpha chatacters with space\n",
    "    if line:\n",
    "        line = regex.sub(' ', line)\n",
    "        tokens = line.lower().split()\n",
    "    \n",
    "    for token in tokens:\n",
    "        # first letter\n",
    "        ch0 = token[0]\n",
    "        update_pi(ch0)\n",
    "\n",
    "        for ch1 in token[1:]:\n",
    "            update_transition(ch0, ch1)\n",
    "            ch0 = ch1\n",
    "\n",
    "# normalize the probabilities\n",
    "pi /= pi.sum()\n",
    "M_bi /= M_bi.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T10:21:37.793572Z",
     "start_time": "2020-08-12T10:21:37.788731Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "3FxNj6V2UdM3"
   },
   "outputs": [],
   "source": [
    "original_message = '''\n",
    "I then lounged down the street and found,\n",
    "as I expected, that there was a mews in a lane which runs down\n",
    "by one wall of the garden. I lent the ostlers a hand in rubbing\n",
    "down their horses, and received in exchange twopence, a glass of\n",
    "half-and-half, two fills of shag tobacco, and as much information\n",
    "as I could desire about Miss Adler, to say nothing of half a dozen\n",
    "other people in the neighbourhood in whom I was not in the least\n",
    "interested, but whose biographies I was compelled to listen to.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T10:21:38.219727Z",
     "start_time": "2020-08-12T10:21:38.210507Z"
    }
   },
   "outputs": [],
   "source": [
    "# encode the message\n",
    "encoded_message = encode_message(original_message)\n",
    "# to decode\n",
    "decode_message(encoded_message, {y:x for x, y in true_mapping.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolutionary algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T10:22:58.015710Z",
     "start_time": "2020-08-12T10:22:58.009486Z"
    }
   },
   "outputs": [],
   "source": [
    "def evolve_offspring(dna_pool, n_children):\n",
    "    '''to create offspring'''\n",
    "    offspring = []\n",
    "    \n",
    "    for dna in dna_pool:\n",
    "        for _ in range(n_children):\n",
    "            copy = dna.copy()\n",
    "            j = np.random.randint(len(copy))\n",
    "            k = np.random.randint(len(copy))\n",
    "\n",
    "            # switch position of alphabet\n",
    "            tmp = copy[j]\n",
    "            copy[j] = copy[k]\n",
    "            copy[k] = tmp\n",
    "            offspring.append(copy)\n",
    "\n",
    "    return offspring + dna_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T10:22:58.310850Z",
     "start_time": "2020-08-12T10:22:58.302998Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Ogy2tFp2UjEW"
   },
   "outputs": [],
   "source": [
    "# initialization\n",
    "num_iters = 300\n",
    "scores = np.zeros(num_iters)\n",
    "best_dna = None\n",
    "best_map = None\n",
    "best_score = -np.inf\n",
    "\n",
    "dna_pool = []\n",
    "for count in range(50):\n",
    "    dna = list(string.ascii_lowercase)\n",
    "    random.shuffle(dna)\n",
    "    dna_pool.append(dna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T10:23:15.046777Z",
     "start_time": "2020-08-12T10:22:58.619448Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(num_iters):\n",
    "    # get offspring from the current dna pool\n",
    "    # skip this during first run\n",
    "    if i > 0:\n",
    "        dna_pool = evolve_offspring(dna_pool, 5)\n",
    " \n",
    "    # calculate score for each dna\n",
    "    dna2score = {}\n",
    "    for dna in dna_pool:\n",
    "        current_map = {k : v for k, v in zip(letters1, dna)}\n",
    "        decoded_message = decode_message(encoded_message, current_map)\n",
    "        score = get_sequence_prob(decoded_message)\n",
    "        # store the score for each map\n",
    "        dna2score[''.join(dna)] = score\n",
    "        \n",
    "        # record the best score\n",
    "        if score > best_score:\n",
    "            best_dna = dna\n",
    "            best_map = current_map\n",
    "            best_score = score\n",
    " \n",
    "    # store average score of current generation\n",
    "    scores[i] = np.mean(list(dna2score.values()))\n",
    "    # keep the best 5 dna from dna_pool (20)\n",
    "    sorted_dna = sorted(dna2score.items(), key=lambda x: x[1], reverse=True)\n",
    "    dna_pool = [list(k) for k, v in sorted_dna[:10]]\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"iter:\", i, \"score:\", scores[i], \"best so far:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T10:23:15.061925Z",
     "start_time": "2020-08-12T10:23:15.048657Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "TBI4r2roUqP9",
    "outputId": "b215627b-1f64-4006-af25-abca5c096406"
   },
   "outputs": [],
   "source": [
    "# decode message using best map\n",
    "decoded_message = decode_message(encoded_message, best_map)\n",
    "print(\"LL of decoded message:\", get_sequence_prob(decoded_message))\n",
    "print(\"LL of true message:\", get_sequence_prob(regex.sub(' ', original_message.lower())))\n",
    "\n",
    "# which letters are wrong?\n",
    "for true, v in true_mapping.items():\n",
    "    pred = best_map[v]\n",
    "    if true != pred:\n",
    "        print(\"true: %s, pred: %s\" % (true, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T10:23:15.076810Z",
     "start_time": "2020-08-12T10:23:15.067092Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "cZqTut0mU-6S",
    "outputId": "0dab349d-218b-475b-919e-d813a7626d5c"
   },
   "outputs": [],
   "source": [
    "# print the final decoded message\n",
    "print(\"Decoded message:\\n\", textwrap.fill(decoded_message))\n",
    "print(\"\\nTrue message:\\n\", original_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Spinner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T14:10:02.737241Z",
     "start_time": "2020-08-13T14:10:02.722801Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T14:38:42.717167Z",
     "start_time": "2020-08-13T14:38:42.709128Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_sample(d):\n",
    "    '''choose a random sample from dictionary where values are the probabilities'''\n",
    "    return \n",
    "\n",
    "def tokenizer(review):\n",
    "    s = review.text.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    return tokens\n",
    "    \n",
    "# def test_spinner():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T14:38:43.983471Z",
     "start_time": "2020-08-13T14:38:43.267118Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_reviews = BeautifulSoup(open('./data/positive.review').read())\n",
    "positive_reviews = positive_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T14:38:45.206485Z",
     "start_time": "2020-08-13T14:38:43.987674Z"
    }
   },
   "outputs": [],
   "source": [
    "# (w1, w3) is the key, [ w2 ] are the values\n",
    "trigrams = {}\n",
    "for review in positive_reviews:\n",
    "    tokens = tokenizer(review)\n",
    "    for i in range(len(tokens) - 2):\n",
    "        k = (tokens[i], tokens[i+2])\n",
    "        if k not in trigrams:\n",
    "            trigrams[k] = []\n",
    "        trigrams[k].append(tokens[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T14:38:45.218310Z",
     "start_time": "2020-08-13T14:38:45.208585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['always', 'always', 'finally', 'never', 'never', 'never']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams['have', 'been']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T14:38:46.153756Z",
     "start_time": "2020-08-13T14:38:45.965144Z"
    }
   },
   "outputs": [],
   "source": [
    "trigram_probabilities = {}\n",
    "for k, words in trigrams.items():\n",
    "    d, n = {w : 0 for w in set(words)}, len(words)     \n",
    "    for w in words:\n",
    "        d[w] += 1\n",
    "    d = {w : v/n for w,v in d.items()}\n",
    "    trigram_probabilities[k] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T14:38:47.041384Z",
     "start_time": "2020-08-13T14:38:47.033186Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'finally': 0.16666666666666666, 'never': 0.5, 'always': 0.3333333333333333}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_probabilities['have', 'been']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T14:48:34.294156Z",
     "start_time": "2020-08-13T14:48:34.278664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: <review_text>\n",
      "Researched a lot before I bought this TV and for the money, this one was hands down the best 32\" LCD.   Also, Amazon had the best price by far and it shipped to my house in a timely manner with no major issues.   \n",
      "I mainly use this TV for my XBOX 360.  You will NOT be disappointed if you are planning on doing the same.   The game mode does enhance the picture a great deal as well.  I provides sort of an Anti Aliasing effect smoothing out some of the rough edges. All in all, after owning the tv for about 2 months now, I couldn't be happier.   To get around the poor speakers, I just plug in a nice set of headphones and games sound GREAT.  \n",
      "I've also hooked up an HD antenna and regular cable.  HD channels have a very good picture over the air.  Some football games almost look better on this than on my 42\" Samsung DLP downstairs with the HD Cable.  There's also a nice Antenna button on the remote which quickly swaps between cable and the Over the Air Antenna.  \n",
      "The only drawbacks to the TV are the 1 component input and the fact you can't use the PIP with the cable connected, only the antenna.  \n",
      "</review_text>\n",
      "\n",
      "Spun:\n",
      "researched a button before i bought this tv and for the web , this splitter was hands down the best 32 '' lcd . granted , amazon had the best price by far - it looked to my house in a timely manner with no major highways . i mainly compared this backpack for my xbox 360. you will always be covered if you are planning on all the same . the 16:9 mode does enhance the picture a huge deal as well . it was sort of an anti aliasing effect smoothing out some of the rough edges . all in all , after receiving the tv for about 2 months now , i could n't be happier . must get around the white speakers so i just plug in a decent set of field and muffled sound great . they can also fold up an external antenna and the store . hd channels have a very good picture of the past . watching football games almost look nice on this than on my 19 '' samsung dlp downstairs given the same cable . he 's also a nice antenna button in the remote which quickly swaps between touchpad and the bridge the air antenna . the only drawbacks to the holes on the 1 component input and the fact you ca n't use the lens with the cable modem , in the space .\n"
     ]
    }
   ],
   "source": [
    "review = random.choice(positive_reviews)\n",
    "print(\"Original:\", review)\n",
    "tokens = tokenizer(review)\n",
    "\n",
    "for i in range(len(tokens) - 2):\n",
    "    # 20% chance of replacement\n",
    "    k = (tokens[i], tokens[i+2])\n",
    "    if (random.random() < 0.5) & (k in trigram_probabilities):\n",
    "        w = np.random.choice([x for x in trigram_probabilities[k].keys()])\n",
    "        tokens[i+1] = w\n",
    "\n",
    "print(\"\\nSpun:\")\n",
    "print(\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Cipher.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
