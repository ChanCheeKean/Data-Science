{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T18:00:09.219839Z",
     "start_time": "2020-10-12T18:00:06.958980Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing as ExpS\n",
    "from statsmodels.tsa.stattools import acovf,acf,pacf,pacf_yw,pacf_ols\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "from statsmodels.tsa.statespace.tools import diff\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima_model import ARMA,ARMAResults,ARIMA,ARIMAResults\n",
    "from statsmodels.tsa.statespace.tools import diff\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from pandas.plotting import lag_plot\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.api import VAR\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting with the Holt-Winters Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/airline_passengers.csv',index_col='Month',parse_dates=True)\n",
    "df.index.freq = 'MS'\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting\n",
    "train_data = df.iloc[:108]\n",
    "test_data = df.iloc[108:]\n",
    "\n",
    "# plotting\n",
    "train_data['Thousands of Passengers'].plot(legend=True,label='TRAIN')\n",
    "test_data['Thousands of Passengers'].plot(legend=True,label='TEST',figsize=(8,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = ExpS(train_data['Thousands of Passengers'], trend='mul', seasonal='mul', seasonal_periods=12).fit()\n",
    "test_predictions = fitted_model.forecast(36).rename('HW Forecast')\n",
    "\n",
    "train_data['Thousands of Passengers'].plot(legend=True, label='TRAIN')\n",
    "test_data['Thousands of Passengers'].plot(legend=True, label='TEST', figsize=(8, 5))\n",
    "test_predictions.plot(legend=True, label='PREDICTION');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmse\n",
    "np.sqrt(mean_squared_error(test_data,test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACF and PACF\n",
    "Before we can investigate <em>autoregression</em> as a modeling tool, we need to look at <em>covariance</em> and <em>correlation</em> as they relate to lagged (shifted) samples of a time series.\n",
    " \n",
    "<br />\n",
    "<strong>\n",
    "<a href='https://en.wikipedia.org/wiki/Autocovariance'>Wikipedia:</a></strong>&nbsp;&nbsp;<font color=black>Autocovariance</font><br>\n",
    "<strong>\n",
    "<a href='https://otexts.com/fpp2/autocorrelation.html'>Forecasting: Principles and Practice</a></strong>&nbsp;&nbsp;<font color=black>Autocorrelation</font><br>\n",
    "<strong>\n",
    "<a href='https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4463.htm'>NIST Statistics Handbook</a></strong>&nbsp;&nbsp;<font color=black>Partial Autocorrelation Plot</font></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocovariance for 1D\n",
    "In a <em>deterministic</em> process, like $y=sin(x)$, we always know the value of $y$ for a given value of $x$. However, in a <em>stochastic</em> process there is always some randomness that prevents us from knowing the value of $y$. Instead, we analyze the past (or <em>lagged</em>) behavior of the system to derive a probabilistic estimate for $\\hat{y}$.\n",
    "\n",
    "One useful descriptor is <em>covariance</em>. When talking about dependent and independent $x$ and $y$ variables, covariance describes how the variance in $x$ relates to the variance in $y$. Here the size of the covariance isn't really important, as $x$ and $y$ may have very different scales. However, if the covariance is positive it means that $x$ and $y$ are changing in the same direction, and may be related.\n",
    "\n",
    "With a time series, $x$ is a fixed interval. Here we want to look at the variance of $y_t$ against lagged or shifted values of $y_{t+k}$\n",
    "\n",
    "For a stationary time series, the autocovariance function for $\\gamma$ (gamma) is given as:\n",
    "\n",
    "${\\displaystyle {\\gamma}_{XX}(t_{1},t_{2})=\\operatorname {Cov} \\left[X_{t_{1}},X_{t_{2}}\\right]=\\operatorname {E} [(X_{t_{1}}-\\mu _{t_{1}})(X_{t_{2}}-\\mu _{t_{2}})]}$\n",
    "\n",
    "We can calculate a specific $\\gamma_k$ with:\n",
    "\n",
    "${\\displaystyle \\gamma_k = \\frac 1 n \\sum\\limits_{t=1}^{n-k} (y_t - \\bar{y})(y_{t+k}-\\bar{y})}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocovariance Example:\n",
    "Say we have a time series with five observations: {13, 5, 11, 12, 9}.<br>\n",
    "We can quickly see that $n = 5$, the mean $\\bar{y} = 10$, and we'll see that the variance $\\sigma^2 = 8$.<br>\n",
    "The following calculations give us our covariance values:\n",
    "<br><br>\n",
    "$\\gamma_0 = \\frac {(13-10)(13-10)+(5-10)(5-10)+(11-10)(11-10)+(12-10)(12-10)+(9-10)(9-10)} 5 = \\frac {40} 5 = 8.0 \\\\\n",
    "\\gamma_1 = \\frac {(13-10)(5-10)+(5-10)(11-10)+(11-10)(12-10)+(12-10)(9-10)} 5 = \\frac {-20} 5 = -4.0 \\\\\n",
    "\\gamma_2 = \\frac {(13-10)(11-10)+(5-10)(12-10)+(11-10)(9-10)} 5 = \\frac {-8} 5 = -1.6 \\\\\n",
    "\\gamma_3 = \\frac {(13-10)(12-10)+(5-10)(9-10)} 5 = \\frac {11} 5 = 2.2 \\\\\n",
    "\\gamma_4 = \\frac {(13-10)(9-10)} 5 = \\frac {-3} 5 = -0.6$\n",
    "<br><br>\n",
    "Note that $\\gamma_0$ is just the population variance $\\sigma^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'a':[13, 5, 11, 12, 9]})\n",
    "acovf(df['a'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unbiased Autocovariance\n",
    "Note that the number of terms in the calculations above are decreasing.<br>Statsmodels can return an \"unbiased\" autocovariance where instead of dividing by $n$ we divide by $n-k$.\n",
    "\n",
    "$\\gamma_0 = \\frac {(13-10)(13-10)+(5-10)(5-10)+(11-10)(11-10)+(12-10)(12-10)+(9-10)(9-10)} {5-0} = \\frac {40} 5 = 8.0 \\\\\n",
    "\\gamma_1 = \\frac {(13-10)(5-10)+(5-10)(11-10)+(11-10)(12-10)+(12-10)(9-10)} {5-1} = \\frac {-20} 4 = -5.0 \\\\\n",
    "\\gamma_2 = \\frac {(13-10)(11-10)+(5-10)(12-10)+(11-10)(9-10)} {5-2} = \\frac {-8} 3 = -2.67 \\\\\n",
    "\\gamma_3 = \\frac {(13-10)(12-10)+(5-10)(9-10)} {5-3} = \\frac {11} 2 = 5.5 \\\\\n",
    "\\gamma_4 = \\frac {(13-10)(9-10)} {5-4} = \\frac {-3} 1 = -3.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acovf(df['a'],unbiased=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation for 1D\n",
    "The correlation $\\rho$ (rho) between two variables $y_1,y_2$ is given as:\n",
    "\n",
    "$\\rho = \\frac {\\operatorname E[(y_1−\\mu_1)(y_2−\\mu_2)]} {\\sigma_{1}\\sigma_{2}} = \\frac {\\operatorname {Cov} (y_1,y_2)} {\\sigma_{1}\\sigma_{2}}$,\n",
    "\n",
    "where $E$ is the expectation operator, $\\mu_{1},\\sigma_{1}$ and $\\mu_{2},\\sigma_{2}$ are the means and standard deviations of $y_1$ and $y_2$.\n",
    "\n",
    "When working with a single variable (i.e. <em>autocorrelation</em>) we would consider $y_1$ to be the original series and $y_2$ a lagged version of it. Note that with autocorrelation we work with $\\bar y$, that is, the full population mean, and <em>not</em> the means of the reduced set of lagged factors (see note below).\n",
    "\n",
    "Thus, the formula for $\\rho_k$ for a time series at lag $k$ is:\n",
    "\n",
    "${\\displaystyle \\rho_k = \\frac {\\sum\\limits_{t=1}^{n-k} (y_t - \\bar{y})(y_{t+k}-\\bar{y})} {\\sum\\limits_{t=1}^{n} (y_t - \\bar{y})^2}}$\n",
    "\n",
    "This can be written in terms of the covariance constant $\\gamma_k$ as:\n",
    "\n",
    "${\\displaystyle \\rho_k = \\frac {\\gamma_k n} {\\gamma_0 n} = \\frac {\\gamma_k} {\\sigma^2}}$\n",
    "\n",
    "For example,<br>\n",
    "$\\rho_4 = \\frac {\\gamma_4} {\\sigma^2} = \\frac{-0.6} {8} = -0.075$\n",
    "\n",
    "Note that ACF values are bound by -1 and 1. That is, ${\\displaystyle -1 \\leq \\rho_k \\leq 1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Autocorrelation\n",
    "Partial autocorrelations measure the linear dependence of one variable after removing the effect of other variable(s) that affect both variables. That is, the partial autocorrelation at lag $k$ is the autocorrelation between $y_t$ and $y_{t+k}$ that is not accounted for by lags $1$ through $k−1$.\n",
    "\n",
    "A common method employs the non-recursive <a href='https://en.wikipedia.org/wiki/Autoregressive_model#Calculation_of_the_AR_parameters'>Yule-Walker Equations</a>:\n",
    "\n",
    "$\\phi_0 = 1\\\\\n",
    "\\phi_1 = \\rho_1 = -0.50\\\\\n",
    "\\phi_2 = \\frac {\\rho_2 - {\\rho_1}^2} {1-{\\rho_1}^2} = \\frac {(-0.20) - {(-0.50)}^2} {1-{(-0.50)}^2}= \\frac {-0.45} {0.75} = -0.60$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $k$ increases, we can solve for $\\phi_k$ using matrix algebra and the <a href='https://en.wikipedia.org/wiki/Levinson_recursion'>Levinson–Durbin recursion</a> algorithm which maps the sample autocorrelations $\\rho$ to a <a href='https://en.wikipedia.org/wiki/Toeplitz_matrix'>Toeplitz</a> diagonal-constant matrix. The full solution is beyond the scope of this course, but the setup is as follows:\n",
    "\n",
    "\n",
    "$\\displaystyle \\begin{pmatrix}\\rho_0&\\rho_1&\\cdots &\\rho_{k-1}\\\\\n",
    "\\rho_1&\\rho_0&\\cdots &\\rho_{k-2}\\\\\n",
    "\\vdots &\\vdots &\\ddots &\\vdots \\\\\n",
    "\\rho_{k-1}&\\rho_{k-2}&\\cdots &\\rho_0\\\\\n",
    "\\end{pmatrix}\\quad \\begin{pmatrix}\\phi_{k1}\\\\\\phi_{k2}\\\\\\vdots\\\\\\phi_{kk}\\end{pmatrix}\n",
    "\\mathbf = \\begin{pmatrix}\\rho_1\\\\\\rho_2\\\\\\vdots\\\\\\rho_k\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacf_yw(df['a'],nlags=4,method='mle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><strong>NOTE:</strong> We passed in <tt><font color=black>method='mle'</font></tt> above in order to use biased ACF coefficients. \"mle\" stands for \"maximum likelihood estimation\". Alternatively we can pass <tt>method='unbiased'</tt> (the statsmodels default):</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacf_yw(df['a'],nlags=4,method='unbiased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Autocorrelation with OLS\n",
    "pacf_ols(df['a'],nlags=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a non-stationary dataset\n",
    "df1 = pd.read_csv('./Data/airline_passengers.csv',index_col='Month',parse_dates=True)\n",
    "df1.index.freq = 'MS'\n",
    "\n",
    "# Load a stationary dataset\n",
    "df2 = pd.read_csv('./Data/DailyTotalFemaleBirths.csv',index_col='Date',parse_dates=True)\n",
    "df2.index.freq = 'D'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACF Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot(df1['Thousands of Passengers']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually this shows evidence of a very strong autocorrelation; as $y_t$ values increase, nearby (lagged) values also increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation at different lags\n",
    "title = 'Autocorrelation: Daily Female Births'\n",
    "lags = 40\n",
    "plot_acf(df2,title=title,lags=lags);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Autocorrelation: Airline Passengers'\n",
    "lags = 40\n",
    "plot_acf(df1,title=title,lags=lags);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PACF Plots\n",
    "Partial autocorrelations work best with stationary data. Let's look first at <strong>Daily Total Female Births</strong>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title='Partial Autocorrelation: Daily Female Births'\n",
    "lags=40\n",
    "plot_pacf(df2,title=title,lags=lags);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To make the Airline Passengers data stationary, we'll first apply differencing:\n",
    "df1['d1'] = diff(df1['Thousands of Passengers'],k_diff=1)\n",
    "title='PACF: Airline Passengers First Difference'\n",
    "lags=40\n",
    "plot_pacf(df1['d1'].dropna(),title=title,lags=np.arange(lags));  # be sure to add .dropna() here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><strong>A NOTE ABOUT AUTOCORRELATION:</strong> Some texts compute lagged correlations using the Pearson Correlation Coefficient given by:<br><br>\n",
    "${\\displaystyle r_{xy}={\\frac {\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})(y_{i}-{\\bar {y}})}{{\\sqrt {\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})^{2}}}{\\sqrt {\\sum _{i=1}^{n}(y_{i}-{\\bar {y}})^{2}}}}}}$\n",
    "\n",
    "These are easily calculated in numpy with <tt>numpy.corrcoef(x,y)</tt> and in Excel with <tt>=CORREL(x,y)</tt>.<br><br>\n",
    "Using our example, $r_0$ is still $1$, but to solve for $r_1$:\n",
    "\n",
    "${\\displaystyle x_1 = [13, 5, 11, 12], \\bar{x_1} = 10.25}$<br><br>\n",
    "$y_1 = [5, 11, 12, 9],\\  \\bar{y_1} = 9.25$<br><br>\n",
    "$r_{{x_1}{y_1}}=\\frac {(13-10.25)(5-9.25)+(5-10.25)(11-9.25)+(11-10.25)(12-9.25)+(12-10.25)(9-9.25)}\n",
    "{\\sqrt{((13-10.25)^2+(5-10.25)^2+(11-10.25)^2+(12-10.25)^2)}\\sqrt{((5-9.25)^2+(11-9.25)^2+(12-9.25)^2+(9-9.25)^2)}} = \\frac {-19.25} {33.38} = -0.577$\n",
    "<br><br>\n",
    "However, there are some shortcomings. Using the Pearson method, the second-to-last term $r_{k-1}$ will always be $1$ and the last term $r_k$ will always be undefined.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA\n",
    "\n",
    "<strong>ARIMA</strong>, or <em>Autoregressive Integrated Moving Average</em> is actually a combination of 3 models:\n",
    "* <strong>AR(p)</strong> Autoregression - a regression model that utilizes the dependent relationship between a current observation and observations over a previous period\n",
    "* <strong>I(d)</strong> Integration - uses differencing of observations (subtracting an observation from an observation at the previous time step) in order to make the time series stationary\n",
    "* <strong>MA(q)</strong> Moving Average - a model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T03:20:54.242015Z",
     "start_time": "2020-07-07T03:20:54.193117Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load a non-stationary dataset\n",
    "df1 = pd.read_csv('./Data/airline_passengers.csv',index_col='Month',parse_dates=True)\n",
    "df1.index.freq = 'MS'\n",
    "\n",
    "# Load a stationary dataset\n",
    "df2 = pd.read_csv('./Data/DailyTotalFemaleBirths.csv',index_col='Date',parse_dates=True)\n",
    "df2.index.freq = 'D'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pmdarima\n",
    "\n",
    "To find the best parameters for Arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T03:20:57.675575Z",
     "start_time": "2020-07-07T03:20:55.322384Z"
    }
   },
   "outputs": [],
   "source": [
    "stepwise_fit = auto_arima(df2['Births'], start_p=0, start_q=0,\n",
    "                          max_p=6, max_q=3, m=12,\n",
    "                          seasonal=False,\n",
    "                          d=None, trace=True,\n",
    "                          error_action='ignore',   \n",
    "                          suppress_warnings=True, \n",
    "                          stepwise=True)\n",
    "\n",
    "stepwise_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T03:20:58.110371Z",
     "start_time": "2020-07-07T03:20:57.679236Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('./Data/TradeInventories.csv',index_col='Date',parse_dates=True)\n",
    "df2.index.freq='MS'\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "formatter = ticker.StrMethodFormatter('{x:,.0f}')\n",
    "\n",
    "title = 'Real Manufacturing and Trade Inventories'\n",
    "ylabel='Chained 2012 Dollars'\n",
    "xlabel='' # we don't really need a label here\n",
    "\n",
    "ax = df2['Inventories'].plot(figsize=(12,5),title=title)\n",
    "ax.autoscale(axis='x',tight=True)\n",
    "ax.set(xlabel=xlabel, ylabel=ylabel)\n",
    "ax.yaxis.set_major_formatter(formatter);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T03:20:58.919671Z",
     "start_time": "2020-07-07T03:20:58.236631Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = seasonal_decompose(df2['Inventories'], model='additive')  # model='add' also works\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T03:21:00.838345Z",
     "start_time": "2020-07-07T03:21:00.829266Z"
    }
   },
   "outputs": [],
   "source": [
    "def adf_test(series,title=''):\n",
    "    \"\"\"\n",
    "    Pass in a time series and an optional title, returns an ADF report\n",
    "    \"\"\"\n",
    "    print(f'Augmented Dickey-Fuller Test: {title}')\n",
    "    result = adfuller(series.dropna(),autolag='AIC') # .dropna() handles differenced data\n",
    "    \n",
    "    labels = ['ADF test statistic','p-value','# lags used','# observations']\n",
    "    out = pd.Series(result[0:4],index=labels)\n",
    "\n",
    "    for key,val in result[4].items():\n",
    "        out[f'critical value ({key})']=val\n",
    "        \n",
    "    print(out.to_string())          # .to_string() removes the line \"dtype: float64\"\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"Strong evidence against the null hypothesis\")\n",
    "        print(\"Reject the null hypothesis\")\n",
    "        print(\"Data has no unit root and is stationary\")\n",
    "    else:\n",
    "        print(\"Weak evidence against the null hypothesis\")\n",
    "        print(\"Fail to reject the null hypothesis\")\n",
    "        print(\"Data has a unit root and is non-stationary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T03:21:01.481734Z",
     "start_time": "2020-07-07T03:21:01.446254Z"
    }
   },
   "outputs": [],
   "source": [
    "adf_test(df2['Inventories'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A <strong>PACF Plot</strong> can reveal recommended AR(p) orders, and an <strong>ACF Plot</strong> can do the same for MA(q) orders.<br>\n",
    "Alternatively, we can compare the stepwise <a href='https://en.wikipedia.org/wiki/Akaike_information_criterion'>Akaike Information Criterion (AIC)</a> values across a set of different (p,q) combinations to choose the best combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T03:21:04.009030Z",
     "start_time": "2020-07-07T03:21:03.758585Z"
    }
   },
   "outputs": [],
   "source": [
    "title = 'Autocorrelation: Real Manufacturing and Trade Inventories'\n",
    "lags = 40\n",
    "plot_acf(df2['Inventories'],title=title,lags=lags);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T03:21:04.706157Z",
     "start_time": "2020-07-07T03:21:04.426577Z"
    }
   },
   "outputs": [],
   "source": [
    "title = 'Partial Autocorrelation: Real Manufacturing and Trade Inventories'\n",
    "lags = 40\n",
    "plot_pacf(df2['Inventories'],title=title,lags=lags);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the AR component should be more important than MA. From the <a href='https://people.duke.edu/~rnau/411arim3.htm'>Duke University Statistical Forecasting site</a>:<br>\n",
    "> <em>If the PACF displays a sharp cutoff while the ACF decays more slowly (i.e., has significant spikes at higher lags), we    say that the stationarized series displays an \"AR signature,\" meaning that the autocorrelation pattern can be explained more    easily by adding AR terms than by adding MA terms.</em><br>\n",
    "\n",
    "Let's take a look at <tt>pmdarima.auto_arima</tt> done stepwise to see if having $p$ and $q$ terms the same still makes sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T03:21:07.675129Z",
     "start_time": "2020-07-07T03:21:06.683168Z"
    }
   },
   "outputs": [],
   "source": [
    "stepwise_fit = auto_arima(df2['Inventories'], start_p=0, start_q=0,\n",
    "                          max_p=2, max_q=2, m=12,\n",
    "                          seasonal=False,\n",
    "                          d=None, trace=True,\n",
    "                          error_action='ignore',   # we don't want to know if an order does not work\n",
    "                          suppress_warnings=True,  # we don't want convergence warnings\n",
    "                          stepwise=True)           # set to stepwise\n",
    "\n",
    "stepwise_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T03:21:08.178125Z",
     "start_time": "2020-07-07T03:21:07.679067Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set one year for testing\n",
    "train = df2.iloc[:252]\n",
    "test = df2.iloc[252:]\n",
    "\n",
    "model = ARIMA(train['Inventories'],order=(1,1,1))\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T03:21:11.117764Z",
     "start_time": "2020-07-07T03:21:11.065634Z"
    }
   },
   "outputs": [],
   "source": [
    "# Obtain predicted values\n",
    "start=len(train)\n",
    "end=len(train)+len(test)-1\n",
    "predictions = results.predict(start=start, end=end, dynamic=False, typ='levels').rename('ARIMA(1,1,1) Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T03:21:12.209283Z",
     "start_time": "2020-07-07T03:21:11.926950Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot predictions against known values\n",
    "title = 'Real Manufacturing and Trade Inventories'\n",
    "ylabel='Chained 2012 Dollars'\n",
    "xlabel='' # we don't really need a label here\n",
    "\n",
    "ax = test['Inventories'].plot(legend=True,figsize=(8,5),title=title)\n",
    "predictions.plot(legend=True)\n",
    "ax.autoscale(axis='x',tight=True)\n",
    "ax.set(xlabel=xlabel, ylabel=ylabel)\n",
    "ax.yaxis.set_major_formatter(formatter);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T03:21:13.904856Z",
     "start_time": "2020-07-07T03:21:13.899287Z"
    }
   },
   "outputs": [],
   "source": [
    "error = mean_squared_error(test['Inventories'], predictions)\n",
    "print(f'ARIMA(1,1,1) MSE Error: {error:11.10}')\n",
    "error = rmse(test['Inventories'], predictions)\n",
    "print(f'ARIMA(1,1,1) RMSE Error: {error:11.10}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where ARIMA accepts the parameters $(p,d,q)$, SARIMA accepts an <em>additional</em> set of parameters $(P,D,Q)m$ that specifically describe the seasonal components of the model. Here $P$, $D$ and $Q$ represent the seasonal regression, differencing and moving average coefficients, and $m$ represents the number of data points (rows) in each seasonal cycle.\n",
    "\n",
    "<a href='https://www.statsmodels.org/stable/statespace.html'>Statsmodels Tutorial:</a></strong>&nbsp;&nbsp;<font color=black>Time Series Analysis by State Space Methods</font></di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T03:21:18.590254Z",
     "start_time": "2020-07-07T03:21:18.543890Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/co2_mm_mlo.csv')\n",
    "df['date'] = pd.to_datetime(dict(year=df['year'], month=df['month'], day=1))\n",
    "df.set_index('date',inplace=True)\n",
    "df.index.freq = 'MS'\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T03:21:19.312212Z",
     "start_time": "2020-07-07T03:21:19.017745Z"
    }
   },
   "outputs": [],
   "source": [
    "title = 'Monthly Mean CO₂ Levels (ppm) over Mauna Loa, Hawaii'\n",
    "ylabel='parts per million'\n",
    "xlabel=''\n",
    "\n",
    "ax = df['interpolated'].plot(figsize=(12,6),title=title)\n",
    "ax.autoscale(axis='x',tight=True)\n",
    "ax.set(xlabel=xlabel, ylabel=ylabel);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T03:22:14.294450Z",
     "start_time": "2020-07-07T03:21:20.045835Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For SARIMA Orders we set seasonal=True and pass in an m value\n",
    "auto_arima(df['interpolated'],seasonal=True,m=12).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T03:22:14.305014Z",
     "start_time": "2020-07-07T03:21:20.960Z"
    }
   },
   "outputs": [],
   "source": [
    "train = df.iloc[:717]\n",
    "test = df.iloc[717:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T03:22:14.306945Z",
     "start_time": "2020-07-07T03:21:21.384Z"
    }
   },
   "outputs": [],
   "source": [
    "model = SARIMAX(train['interpolated'],order=(0,1,1),seasonal_order=(1,0,1,12))\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T03:22:14.312009Z",
     "start_time": "2020-07-07T03:21:22.240Z"
    }
   },
   "outputs": [],
   "source": [
    "# Obtain predicted values\n",
    "start=len(train)\n",
    "end=len(train)+len(test)-1\n",
    "predictions = results.predict(start=start, \n",
    "                              end=end, \n",
    "                              dynamic=False, \n",
    "                              typ='levels').rename('SARIMA(0,1,3)(1,0,1,12) Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing <tt>dynamic=False</tt> means that forecasts at each point are generated using the full history up to that point (all lagged values).\n",
    "\n",
    "Passing <tt>typ='levels'</tt> predicts the levels of the original endogenous variables. If we'd used the default <tt>typ='linear'</tt> we would have seen linear predictions in terms of the differenced endogenous variables.\n",
    "\n",
    "For more information on these arguments visit https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima_model.ARIMAResults.predict.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T03:22:14.320789Z",
     "start_time": "2020-07-07T03:21:23.587Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot predictions against known values\n",
    "title = 'Monthly Mean CO₂ Levels (ppm) over Mauna Loa, Hawaii'\n",
    "ylabel='parts per million'\n",
    "xlabel=''\n",
    "\n",
    "ax = test['interpolated'].plot(legend=True,figsize=(12,6),title=title)\n",
    "predictions.plot(legend=True)\n",
    "ax.autoscale(axis='x',tight=True)\n",
    "ax.set(xlabel=xlabel, ylabel=ylabel);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce the idea that external factors (environmental, economic, etc.) can also influence a time series, and be used in forecasting.\n",
    "\n",
    "\n",
    "<strong>\n",
    "<a href='https://www.statsmodels.org/stable/statespace.html'>Statsmodels Tutorial:</a></strong>&nbsp;&nbsp;<font color=black>Time Series Analysis by State Space Methods</font><br>\n",
    "<strong>\n",
    "<a href='https://www.statsmodels.org/devel/examples/notebooks/generated/statespace_sarimax_stata.html'>Statsmodels Example:</a></strong>&nbsp;&nbsp;<font color=black>SARIMAX</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('./Data/RestaurantVisitors.csv',index_col='date',parse_dates=True)\n",
    "df.index.freq = 'D'\n",
    "df1 = df.dropna()\n",
    "cols = ['rest1','rest2','rest3','rest4','total']\n",
    "df1[cols] = df1[cols].astype(int)\n",
    "df1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set four weeks for testing\n",
    "train = df1.iloc[:436]\n",
    "test = df1.iloc[436:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title='Restaurant Visitors'\n",
    "ylabel='Visitors per day'\n",
    "xlabel='' # we don't really need a label here\n",
    "\n",
    "ax = df1['total'].plot(figsize=(16,5),title=title)\n",
    "ax.autoscale(axis='x',tight=True)\n",
    "ax.set(xlabel=xlabel, ylabel=ylabel);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For SARIMA Orders we set seasonal=True and pass in an m value\n",
    "auto_arima(df1['total'],seasonal=True,m=7).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sarimax = SARIMAX(train['total'], \n",
    "                        exog=train['holiday'],\n",
    "                        order=(1,0,0),\n",
    "                        seasonal_order=(2,0,0,7),\n",
    "                        enforce_invertibility=False)\n",
    "results_sarimax = model_sarimax.fit()\n",
    "results_sarimax.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain predicted values\n",
    "start=len(train)\n",
    "end=len(train)+len(test)-1\n",
    "exog_forecast = test[['holiday']]  # requires two brackets to yield a shape of (35,1)\n",
    "predictions_sarimax = results_sarimax.predict(start=start, \n",
    "                                              end=end, \n",
    "                                              exog=exog_forecast).rename('SARIMAX(1,0,0)(2,0,0,7) Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions against known values\n",
    "title='Restaurant Visitors'\n",
    "ylabel='Visitors per day'\n",
    "xlabel=''\n",
    "\n",
    "ax = test['total'].plot(legend=True,figsize=(12,6),title=title)\n",
    "predictions_sarimax.plot(legend=True)\n",
    "ax.autoscale(axis='x',tight=True)\n",
    "ax.set(xlabel=xlabel, ylabel=ylabel)\n",
    "for x in test.query('holiday==1').index: \n",
    "    ax.axvline(x=x, color='k', alpha=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted values for sarima\n",
    "model_sarima = SARIMAX(train['total'], \n",
    "                       order=(1,0,0), \n",
    "                       seasonal_order=(2,0,0,7), \n",
    "                       enforce_invertibility=False)\n",
    "results_sarima = model_sarima.fit()\n",
    "predictions_sarima = results_sarima.predict(start=start, end=end, dynamic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tools.eval_measures import mse,rmse\n",
    "\n",
    "# Print values from SARIMA above\n",
    "error1 = mse(test['total'], predictions_sarima)\n",
    "error2 = rmse(test['total'], predictions_sarima)\n",
    "\n",
    "print(f'SARIMA(1,0,0)(2,0,0,7) MSE Error: {error1:11.10}')\n",
    "print(f'SARIMA(1,0,0)(2,0,0,7) RMSE Error: {error2:11.10}')\n",
    "print()\n",
    "\n",
    "# Print new SARIMAX values\n",
    "error1x = mse(test['total'], predictions_sarimax)\n",
    "error2x = rmse(test['total'], predictions_sarimax)\n",
    "\n",
    "print(f'SARIMAX(1,0,0)(2,0,0,7) MSE Error: {error1x:11.10}')\n",
    "print(f'SARIMAX(1,0,0)(2,0,0,7) RMSE Error: {error2x:11.10}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector-AutoRegression\n",
    "\n",
    "there are some cases where variables affect each other. <a href='https://otexts.com/fpp2/VAR.html'>Forecasting: Principles and Practice</a> describes a case where changes in personal consumption expenditures $C_t$ were forecast based on changes in personal disposable income $I_t$.\n",
    "> However, in this case a bi-directional relationship may be more suitable: an increase in $I_t$ will lead to an increase in $C_t$ and vice versa.<br>An example of such a situation occurred in Australia during the Global Financial Crisis of 2008–2009. The Australian government issued stimulus packages that included cash payments in December 2008, just in time for Christmas spending. As a result, retailers reported strong sales and the economy was stimulated. Consequently, incomes increased.\n",
    "\n",
    "Aside from investigating multivariate time series, vector autoregression is used for\n",
    "* <a href='https://www.statsmodels.org/devel/vector_ar.html#impulse-response-analysis'>Impulse Response Analysis</a> which involves the response of one variable to a sudden but temporary change in another variable\n",
    "* <a href='https://www.statsmodels.org/devel/vector_ar.html#forecast-error-variance-decomposition-fevd'>Forecast Error Variance Decomposition (FEVD)</a> where the proportion of the forecast variance of one variable is attributed to the effect of other variables\n",
    "* <a href='https://www.statsmodels.org/devel/vector_ar.html#dynamic-vector-autoregressions'>Dynamic Vector Autoregressions</a> used for estimating a moving-window regression for the purposes of making forecasts throughout the data sample\n",
    "a\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $y_{t} = c + \\phi_{1}y_{t-1} + \\phi_{2}y_{t-2} + \\dots + \\phi_{p}y_{t-p} + \\varepsilon_{t}$\n",
    "\n",
    "where $c$ is a constant, $\\phi_{1}$ and $\\phi_{2}$ are lag coefficients up to order $p$, and $\\varepsilon_{t}$ is white noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:04:27.531749Z",
     "start_time": "2020-07-06T16:04:27.501939Z"
    }
   },
   "source": [
    "A $K$-dimensional VAR model of order $p$, denoted <strong>VAR(p)</strong>, considers each variable $y_K$ in the system.<br>\n",
    "\n",
    "For example, The system of equations for a 2-dimensional VAR(1) model is:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$y_{1,t} = c_1 + \\phi_{11,1}y_{1,t-1} + \\phi_{12,1}y_{2,t-1} + \\varepsilon_{1,t}$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$y_{2,t} = c_2 + \\phi_{21,1}y_{1,t-1} + \\phi_{22,1}y_{2,t-1} + \\varepsilon_{2,t}$\n",
    "\n",
    "where the coefficient $\\phi_{ii,l}$ captures the influence of the $l$th lag of variable $y_i$ on itself,<br>\n",
    "the coefficient $\\phi_{ij,l}$ captures the influence of the $l$th lag of variable $y_j$ on $y_i$,<br>\n",
    "and $\\varepsilon_{1,t}$ and $\\varepsilon_{2,t}$ are white noise processes that may be correlated.<br>\n",
    "\n",
    "Carrying this further, the system of equations for a 2-dimensional VAR(3) model is:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$y_{1,t} = c_1 + \\phi_{11,1}y_{1,t-1} + \\phi_{12,1}y_{2,t-1} + \\phi_{11,2}y_{1,t-2} + \\phi_{12,2}y_{2,t-2} + \\phi_{11,3}y_{1,t-3} + \\phi_{12,3}y_{2,t-3} + \\varepsilon_{1,t}$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$y_{2,t} = c_2 + \\phi_{21,1}y_{1,t-1} + \\phi_{22,1}y_{2,t-1} + \\phi_{21,2}y_{1,t-2} + \\phi_{22,2}y_{2,t-2} + \\phi_{21,3}y_{1,t-3} + \\phi_{22,3}y_{2,t-3} + \\varepsilon_{2,t}$<br><br>\n",
    "\n",
    "and the system of equations for a 3-dimensional VAR(2) model is:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$y_{1,t} = c_1 + \\phi_{11,1}y_{1,t-1} + \\phi_{12,1}y_{2,t-1} + \\phi_{13,1}y_{3,t-1} + \\phi_{11,2}y_{1,t-2} + \\phi_{12,2}y_{2,t-2} + \\phi_{13,2}y_{3,t-2} + \\varepsilon_{1,t}$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$y_{2,t} = c_2 + \\phi_{21,1}y_{1,t-1} + \\phi_{22,1}y_{2,t-1} + \\phi_{23,1}y_{3,t-1} + \\phi_{21,2}y_{1,t-2} + \\phi_{22,2}y_{2,t-2} + \\phi_{23,2}y_{3,t-2} + \\varepsilon_{2,t}$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$y_{3,t} = c_3 + \\phi_{31,1}y_{1,t-1} + \\phi_{32,1}y_{2,t-1} + \\phi_{33,1}y_{3,t-1} + \\phi_{31,2}y_{1,t-2} + \\phi_{32,2}y_{2,t-2} + \\phi_{33,2}y_{3,t-2} + \\varepsilon_{3,t}$<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:05:49.674634Z",
     "start_time": "2020-07-06T16:05:49.602616Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "df = pd.read_csv('./Data/M2SLMoneyStock.csv',index_col=0, parse_dates=True)\n",
    "df.index.freq = 'MS'\n",
    "\n",
    "sp = pd.read_csv('./Data/PCEPersonalSpending.csv',index_col=0, parse_dates=True)\n",
    "sp.index.freq = 'MS'\n",
    "\n",
    "df = df.join(sp)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:06:05.073618Z",
     "start_time": "2020-07-06T16:06:04.726774Z"
    }
   },
   "outputs": [],
   "source": [
    "title = 'M2 Money Stock vs. Personal Consumption Expenditures'\n",
    "ylabel='Billions of dollars'\n",
    "xlabel=''\n",
    "\n",
    "ax = df['Spending'].plot(figsize=(12,5),title=title,legend=True)\n",
    "ax.autoscale(axis='x',tight=True)\n",
    "ax.set(xlabel=xlabel, ylabel=ylabel)\n",
    "df['Money'].plot(legend=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:07:04.856267Z",
     "start_time": "2020-07-06T16:07:04.803777Z"
    }
   },
   "outputs": [],
   "source": [
    "adf_test(df['Money'],title='Money')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:07:10.464710Z",
     "start_time": "2020-07-06T16:07:10.434811Z"
    }
   },
   "outputs": [],
   "source": [
    "adf_test(df['Spending'], title='Spending')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither variable is stationary, so we'll take a first order difference of the entire DataFrame and re-run the augmented Dickey-Fuller tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:08:04.911115Z",
     "start_time": "2020-07-06T16:08:04.854461Z"
    }
   },
   "outputs": [],
   "source": [
    "df_transformed = df.diff()\n",
    "df_transformed = df_transformed.dropna()\n",
    "adf_test(df_transformed['Money'], title='MoneyFirstDiff')\n",
    "print()\n",
    "adf_test(df_transformed['Spending'], title='SpendingFirstDiff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:08:14.018502Z",
     "start_time": "2020-07-06T16:08:14.010410Z"
    }
   },
   "source": [
    "Since Money is not yet stationary, apply second order differencing to both series so they retain the same number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:08:33.489903Z",
     "start_time": "2020-07-06T16:08:33.432321Z"
    }
   },
   "outputs": [],
   "source": [
    " df_transformed = df_transformed.diff().dropna()\n",
    "adf_test(df_transformed['Money'], title='MoneySecondDiff')\n",
    "print()\n",
    "adf_test(df_transformed['Spending'], title='SpendingSecondDiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:08:40.363022Z",
     "start_time": "2020-07-06T16:08:40.350509Z"
    }
   },
   "outputs": [],
   "source": [
    "# stationary data\n",
    "df_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:09:01.534474Z",
     "start_time": "2020-07-06T16:09:01.528306Z"
    }
   },
   "outputs": [],
   "source": [
    "nobs=12\n",
    "train, test = df_transformed[0:-nobs], df_transformed[-nobs:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:09:45.221457Z",
     "start_time": "2020-07-06T16:09:45.152626Z"
    }
   },
   "outputs": [],
   "source": [
    "# VAR Model Order Selection\n",
    "# Pyramid auto can't be applied here\n",
    "for i in [1,2,3,4,5,6,7]:\n",
    "    model = VAR(train)\n",
    "    results = model.fit(i)\n",
    "    print('Order =', i)\n",
    "    print('AIC: ', results.aic)\n",
    "    print('BIC: ', results.bic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAR(5) model seems to return the lowest combined scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:10:14.914929Z",
     "start_time": "2020-07-06T16:10:14.884000Z"
    }
   },
   "outputs": [],
   "source": [
    "results = model.fit(5)\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:10:45.889116Z",
     "start_time": "2020-07-06T16:10:45.882117Z"
    }
   },
   "outputs": [],
   "source": [
    "# lagged order is 5\n",
    "results.k_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:10:51.380337Z",
     "start_time": "2020-07-06T16:10:51.370976Z"
    }
   },
   "outputs": [],
   "source": [
    "# forecasted result\n",
    "z = results.forecast(y=train.values[-results.k_ar:], steps=12)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:11:26.663910Z",
     "start_time": "2020-07-06T16:11:26.648898Z"
    }
   },
   "outputs": [],
   "source": [
    "df_forecast = pd.DataFrame(z, index=pd.date_range('1/1/2015', periods=12, freq='MS'), columns=['Money2d','Spending2d'])\n",
    "df_forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:12:51.014161Z",
     "start_time": "2020-07-06T16:12:50.984317Z"
    }
   },
   "outputs": [],
   "source": [
    "# invert transformation due to 2nd order difference\n",
    "\n",
    "# Add the most recent first difference from the training side of the original dataset to the forecast cumulative sum\n",
    "df_forecast['Money1d'] = (df['Money'].iloc[-nobs-1]-df['Money'].iloc[-nobs-2]) + df_forecast['Money2d'].cumsum()\n",
    "\n",
    "# Now build the forecast values from the first difference set\n",
    "df_forecast['MoneyForecast'] = df['Money'].iloc[-nobs-1] + df_forecast['Money1d'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:13:01.047868Z",
     "start_time": "2020-07-06T16:13:01.036349Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add the most recent first difference from the training side of the original dataset to the forecast cumulative sum\n",
    "df_forecast['Spending1d'] = (df['Spending'].iloc[-nobs-1]-df['Spending'].iloc[-nobs-2]) + df_forecast['Spending2d'].cumsum()\n",
    "\n",
    "# Now build the forecast values from the first difference set\n",
    "df_forecast['SpendingForecast'] = df['Spending'].iloc[-nobs-1] + df_forecast['Spending1d'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:13:30.259223Z",
     "start_time": "2020-07-06T16:13:30.244038Z"
    }
   },
   "outputs": [],
   "source": [
    "df_forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:15:44.710833Z",
     "start_time": "2020-07-06T16:15:44.310257Z"
    }
   },
   "outputs": [],
   "source": [
    "results.plot_forecast(12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:15:57.152150Z",
     "start_time": "2020-07-06T16:15:56.891356Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Money'][-nobs:].plot(figsize=(12,5),legend=True).autoscale(axis='x',tight=True)\n",
    "df_forecast['MoneyForecast'].plot(legend=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:16:09.408036Z",
     "start_time": "2020-07-06T16:16:09.178377Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Spending'][-nobs:].plot(figsize=(12,5),legend=True).autoscale(axis='x',tight=True)\n",
    "df_forecast['SpendingForecast'].plot(legend=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:17:51.135863Z",
     "start_time": "2020-07-06T16:17:51.125957Z"
    }
   },
   "outputs": [],
   "source": [
    "RMSE1 = rmse(df['Money'][-nobs:], df_forecast['MoneyForecast'])\n",
    "print(f'Money VAR(5) RMSE: {RMSE1:.3f}')\n",
    "\n",
    "RMSE2 = rmse(df['Spending'][-nobs:], df_forecast['SpendingForecast'])\n",
    "print(f'Spending VAR(5) RMSE: {RMSE2:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VARMA (p,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:59:16.568956Z",
     "start_time": "2020-07-06T16:59:16.563662Z"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.varmax import VARMAX, VARMAXResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:59:30.945082Z",
     "start_time": "2020-07-06T16:59:30.933325Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T17:01:46.508327Z",
     "start_time": "2020-07-06T17:01:46.492242Z"
    }
   },
   "outputs": [],
   "source": [
    "df_transformed = df.diff().diff()\n",
    "df_transformed = df_transformed.dropna()\n",
    "df_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T17:02:56.181527Z",
     "start_time": "2020-07-06T17:02:56.178473Z"
    }
   },
   "outputs": [],
   "source": [
    "# can use pmdarima here\n",
    "auto_arima(df_transformed['Money'],maxiter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T17:03:18.859896Z",
     "start_time": "2020-07-06T17:03:12.436330Z"
    }
   },
   "outputs": [],
   "source": [
    "nobs=12\n",
    "train, test = df_transformed[0:-nobs], df_transformed[-nobs:]\n",
    "model = VARMAX(train, order=(1,2), trend='c')\n",
    "results = model.fit(maxiter=1000, disp=False)\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T17:03:32.464149Z",
     "start_time": "2020-07-06T17:03:32.426503Z"
    }
   },
   "outputs": [],
   "source": [
    "df_forecast = results.forecast(12)\n",
    "df_forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T17:03:51.663329Z",
     "start_time": "2020-07-06T17:03:51.653258Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add the most recent first difference from the training side of the original dataset to the forecast cumulative sum\n",
    "df_forecast['Money1d'] = (df['Money'].iloc[-nobs-1]-df['Money'].iloc[-nobs-2]) + df_forecast['Money'].cumsum()\n",
    "\n",
    "# Now build the forecast values from the first difference set\n",
    "df_forecast['MoneyForecast'] = df['Money'].iloc[-nobs-1] + df_forecast['Money'].cumsum()\n",
    "\n",
    "# Add the most recent first difference from the training side of the original dataset to the forecast cumulative sum\n",
    "df_forecast['Spending1d'] = (df['Spending'].iloc[-nobs-1]-df['Spending'].iloc[-nobs-2]) + df_forecast['Spending'].cumsum()\n",
    "\n",
    "# Now build the forecast values from the first difference set\n",
    "df_forecast['SpendingForecast'] = df['Spending'].iloc[-nobs-1] + df_forecast['Spending'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T17:03:57.101466Z",
     "start_time": "2020-07-06T17:03:57.085571Z"
    }
   },
   "outputs": [],
   "source": [
    "df_forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T17:04:04.352733Z",
     "start_time": "2020-07-06T17:04:04.335992Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([df.iloc[-12:],df_forecast[['MoneyForecast','SpendingForecast']]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T17:04:48.654322Z",
     "start_time": "2020-07-06T17:04:48.412635Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Money'][-nobs:].plot(figsize=(12,5),legend=True).autoscale(axis='x',tight=True)\n",
    "df_forecast['MoneyForecast'].plot(legend=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T17:04:57.096214Z",
     "start_time": "2020-07-06T17:04:56.875520Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Spending'][-nobs:].plot(figsize=(12,5),legend=True).autoscale(axis='x',tight=True)\n",
    "df_forecast['SpendingForecast'].plot(legend=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T17:05:10.099234Z",
     "start_time": "2020-07-06T17:05:10.091797Z"
    }
   },
   "outputs": [],
   "source": [
    "RMSE1 = rmse(df['Money'][-nobs:], df_forecast['MoneyForecast'])\n",
    "print(f'Money VAR(5) RMSE: {RMSE1:.3f}')\n",
    "\n",
    "RMSE2 = rmse(df['Spending'][-nobs:], df_forecast['SpendingForecast'])\n",
    "print(f'Spending VAR(5) RMSE: {RMSE2:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>CONCLUSION:</strong> It looks like the VARMA(1,2) model did a relatively poor job compared to simpler alternatives. This tells us that there is little or no interdepence between Money Stock and Personal Consumption Expenditures, at least for the timespan we investigated. This is helpful! By fitting a model and getting poor results we know more about the data than we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
